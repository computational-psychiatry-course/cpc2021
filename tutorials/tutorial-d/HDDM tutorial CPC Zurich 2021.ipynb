{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HDDM tutorial CPC Zurich 2021.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPIETeoAt1XKl0wxG2DPis4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"K8BYv_e2ADxT"},"source":["# Intro\n","\n","This HDDM tutorial was written by Jae-Young Son (jae@brown.edu, www.jaeyoungson.com) for the Computational Psychiatry Course @ Zurich 2021. Please feel free to distribute these free and open materials to interested peers (yay, open science!), but kindly keep this attribution notice in place.\n","\n","The HDDM GitHub page can be found [here](https://github.com/hddm-devs/hddm).  \n","The [online documentation](http://ski.clps.brown.edu/hddm_docs/) is as good as a textbook.  \n","If you need help, the [mailing list](https://groups.google.com/g/hddm-users) is very responsive.\n","\n","Here are some example of studies I've run using HDDM (with scripts):  \n","Manuscript: https://www.nature.com/articles/s41598-019-48050-2  \n","Data and scripts: https://osf.io/8ka47/"]},{"cell_type":"markdown","metadata":{"id":"uX8WJSh3lYL3"},"source":["# Setup\n","\n","There's a small amount of idiosyncratic setup needed to get Google Colab to work with HDDM, so let's do that now."]},{"cell_type":"code","metadata":{"id":"x2LOIL_dlCz8"},"source":["# These packages are not available to Google Colab by default, so we must install them ourselves\n","!pip install pymc\n","!pip install kabuki\n","!pip install hddm\n","\n","# Authorize this notebook to open files in your Google Drive\n","# In the code output below, you'll be prompted to open a link, log into your Google Drive,\n","#   and then copy/paste the authorization code into the text box (also in the code output)\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GUi0dp6YoNhv"},"source":["Let's now import all of the packages needed to run our analyses."]},{"cell_type":"code","metadata":{"id":"rx6IZuDglXPH"},"source":["import matplotlib\n","# If using a computing cluster, uncomment the following line & run before importing matplotlib.pyplot or pylab\n","# matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","import hddm\n","import pandas as pd\n","import pickle\n","from patsy import dmatrix\n","from kabuki.analyze import gelman_rubin\n","from kabuki.analyze import post_pred_gen\n","from kabuki.utils import concat_models\n","import pathlib"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AXiwzchHoibK"},"source":["There are a few things you'll want to change manually here, based on the dataset you're working with and where it lives."]},{"cell_type":"code","metadata":{"id":"BzYFxFHPourm"},"source":["# Point Google Colab to the right place in your Google Drive\n","basepath = '/content/drive/My Drive/Colab Notebooks/hddm_cpc_2021'\n","\n","# Check whether save directories exist; if not, create them\n","pathlib.Path(basepath+'/models/').mkdir(parents=True, exist_ok=True)\n","pathlib.Path(basepath+'/results/').mkdir(parents=True, exist_ok=True)\n","pathlib.Path(basepath+'/ppc/').mkdir(parents=True, exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UTe8wEhTpLN2"},"source":["# Load data\n","\n","Now, let's load some data. Note that at minimum, you need three columns in your CSV:\n","\n","1. `subj_idx`: a unique numeric identifier for each subject\n","2. `response`: the subject's choice on a particular trial (`0`=lower boundary, `1`=upper boundary)\n","3. `rt`: the reaction time associated with the subject's choice\n","\n","You will likely have more columns indicating different experimental manipulations, conditions, etc. Those can be named anything you want, but make sure to format your data in the same way you'd format data for mixed-effects regression models (i.e., one row corresponds to one trial, and each subject has undergone many trials)."]},{"cell_type":"code","metadata":{"id":"u4fYKx2TqSES"},"source":["# Load data from csv file\n","study3_data = hddm.load_csv(basepath+'/JustCon3_ddm_tutorial.csv')\n","study4_data = hddm.load_csv(basepath+'/JustCon4_ddm_tutorial.csv')\n","\n","data = (\n","    # Combine dataframes\n","    pd.concat([study3_data, study4_data], ignore_index=True)\n","    # For tutorial purposes, only looking at one condition\n","    .query('offer == \"unfair\"')\n","    # Personal pet peeve\n","    .reset_index(drop=True)\n","  )\n","\n","# Check to make sure it's the data you expect/want\n","print(data.head(5))\n","data.tail(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pYPeQCHfXlcq"},"source":["# Fitting a \"vanilla\" model\n","\n","The simplest possible model is one in which we estimate the value of the main parameters (drift, threshold, non-decision time, and sometimes bias), which do not vary by condition. For the purpose of illustration, we'll try that out now.\n","\n","In many cases, you won't need to estimate bias. It adds another layer of unnecessary computational complexity, and substantially slows down your sampling speed. However, given our task design (value-based decisions about social punishment), it is probably a good idea to do so."]},{"cell_type":"code","metadata":{"id":"wbJof_RkXk_g"},"source":["# For illustration, restrict to data where the participant was the victim, and\n","# was deciding by themselves (not in a group)\n","model_vanilla = hddm.HDDM(\n","    data.query('study == \"Victim\" & phase == \"alone\"'),\n","    bias=True\n",")\n","\n","# Find a good starting point, which helps with convergence\n","model_vanilla.find_starting_values()\n","\n","# Run the MCMC to get samples\n","model_vanilla.sample(100, burn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IZnB5Hu5q5Y1"},"source":["## Saving and loading models\n","\n","Since it can take *quite* some time to run MCMC, we're not going to be running full-length chains to convergence â€“ or else we'd be stuck here all day.\n","\n","For the same reason, when you're running chains to convergence, you'll want to save the results of your computation. Otherwise, you'll have to rerun the models every time you want to do anything useful with HDDM... and that gets very frustrating and time-consuming.\n","\n","You can save your models to disk using the template below."]},{"cell_type":"code","metadata":{"id":"WW8S-lWftTOJ"},"source":["model_vanilla = hddm.HDDM(\n","    data.query('study == \"Victim\" & phase == \"alone\"'),\n","    bias=True\n",")\n","model_vanilla.find_starting_values()\n","\n","model_vanilla.sample(50, burn=0, dbname=basepath+'/models/vanilla_traces.db', db='pickle')\n","model_vanilla.save(basepath+'/models/vanilla_traces')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kAE452xdzkHf"},"source":["You can subsequently load the model (and resume working with it in a new Python session) using the following template:"]},{"cell_type":"code","metadata":{"id":"5Ep1aVaYt0LS"},"source":["# Save a copy of the same model under a different name\n","model_vanilla.save(basepath+'/models/test_traces')\n","\n","# Now load the model and plot one of the posteriors to make sure it worked\n","model_test = hddm.load(basepath+'/models/test_traces')\n","model_test.plot_posteriors(['v'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MnCCMdkaz_BF"},"source":["In the past, I've sometimes found that HDDM can be a little bit fussy about saving or loading models. To be honest, I also prefer using R over Python for analysis. You can therefore export the model traces to a CSV, which you can subsequently open/analyze in any software. It also means that you can share the full model results with a collaborator who (for example) wants to analyze the data, but doesn't want to go through the trouble of installing HDDM. See below to see how.\n","\n","Bonus: the CSV has 50 rows (not including the header). Why?"]},{"cell_type":"code","metadata":{"id":"KaCXqGMfz-gA"},"source":["model_vanilla.get_traces().to_csv(basepath+'/results/vanilla_traces.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NVrczo3Yb9_-"},"source":["## Visual convergence check\n","\n","Every parameter in the model is estimated jointly with every other parameter in the model. Therefore, if even one parameter fails to converge, it prohibits us from examining any of the other parameters.\n","\n","The fastest (but least reliable) method for checking convergence is to simply plot the posteriors. You want the the traceplot to look like a fuzzy caterpillar (why?), with no obvious trends over samples. You want the autocorrection to be as flat and close to 0 as possible (why?). Finally, you want the histogram to look approximately normal (why?)."]},{"cell_type":"code","metadata":{"id":"-ujBMXRQcqqQ"},"source":["model_vanilla.plot_posteriors()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a8_KoooSfkYs"},"source":["## A more formal convergence test\n","\n","What is convergence? If we run many MCMCs, we want each chain to \"settle\" on the same answer. So, we need some statistic that will tell us to what degree each chain has converged onto the same answer.\n","\n","The Gelman-Rubin statistic (also known as r-hat $\\hat{R}$) is basically a ratio, which compares the amount of variance within a particular chain against the amount of variance in the other chains. If the ratio is close to `1`, that suggests that the variance is comparable across all chains, suggesting that all chains have converged onto the same answer.\n","\n","As with the posterior plots, there will be a $\\hat{R}$ statistic for every parameter estimated. Every single one must satisfy a criterion. [Andrew Gelman has suggested](https://www.tandfonline.com/doi/abs/10.1080/10618600.1998.10474787) a criterion of $\\hat{R} < 1.2$, and I prefer to use $\\hat{R} < 1.1$ in my own work when feasible.\n","\n","In order to compute $\\hat{R}$, we need to run multiple chains. Below, we can see an implementation where we run a `for` loop, since we don't have access to parallel processing. If you're running this on your university's computing cluster, you could try runnning a batch job (e.g., using job arrays on SLURM), where each job is one chain. That allows you to run all chains at the same time, in a pretty straightforward way.\n","\n","As you look at the code below, ask yourself: how is this code similar to what we ran before? What is different?"]},{"cell_type":"code","metadata":{"id":"JIrfzC6ShLX_"},"source":["# Initialize empty list\n","models_vanilla = []\n","\n","for i in range(5):\n","  model_vanilla = hddm.HDDM(\n","      data.query('study == \"Victim\" & phase == \"alone\"'),\n","      bias=True\n","  )\n","  model_vanilla.find_starting_values()\n","  # To save time, keep the sample numbers absurdly low\n","  model_vanilla.sample(20, burn=0,\n","                       dbname=basepath+'/models/vanilla_rhat'+'_%s.db'%i,\n","                       db='pickle')\n","  # Add the latest estimates to the list\n","  models_vanilla.append(model_vanilla)\n","\n","# Save traces to CSV\n","(\n","    concat_models(models_vanilla)\n","    .get_traces()\n","    .to_csv(basepath+'/results/vanilla_rhat_traces.csv')\n",")\n","\n","# Return the Gelman-Rubin statistics\n","gelman_rubin(models_vanilla)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l3wOWdYWLr24"},"source":["You can also save these statistics as a CSV, just to have readily on hand."]},{"cell_type":"code","metadata":{"id":"JWzK4nD8Lz5b"},"source":["(\n","    pd.DataFrame.from_dict(gelman_rubin(models_vanilla), orient='index')\n","    .to_csv(basepath+'/results/vanilla_r_hat.csv')\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HQrv7n821192"},"source":["## Hypothesis testing\n","\n","In a Bayesian estimation framework, hypothesis testing is very straightforward. Since we have a posterior distribution, we can simply see what proportion of the posterior falls above (or below) some criterion of interest. If the proportion is greater than some criterion of interest (95% is often used to mimick the frequentist convention of $p < 0.05$), then we conclude that the parameter value is \"significant.\"\n","\n","In a vanilla model like this, we might simply want to test the hypothesis that the drift rate was less than `0`. If at least 95% of the traces are less than `0`, we'll consider our hypothesis supported.\n","\n","***NOTE:*** You might be tempted to simply run a t-test on the traces, but it is generally NOT appropriate to subject the posteriors to frequentist tests.\n","\n","***NOTE:*** It is not appropriate to perform hypothesis testing until we have checked for convergence. If the chain(s) have not converged, we cannot be confident in our parameter estimates, which means it is not appropriate to perform hypothesis testing on them."]},{"cell_type":"code","metadata":{"id":"SQhHzW793exw"},"source":["# We can see all \"nodes\" (parameters) contained in the model\n","print(model_vanilla.nodes_db.node)\n","\n","# We want to pull out the drift rate parameter\n","vanilla_v = model_vanilla.nodes_db.node['v']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jVyMhWH25ZFC"},"source":["# Let's plot the posterior\n","hddm.analyze.plot_posterior_nodes([vanilla_v])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WI33nXvR5arC"},"source":["# What proportion of that posterior is greater than 0?\n","\n","# We can see whether each of the traces is greater than 0\n","print(vanilla_v.trace() < 0)\n","\n","# The mean of ^ will tell us the proportion\n","print((vanilla_v.trace() < 0).mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y_RtNdM-1t8L"},"source":["# Multiple (between-subjects) conditions\n","\n","Most of the time, we're interested in testing the effects of different conditions. This can be done with a simple extension of the syntax we've already covered/used. In this case, we'll compare parameter estimates for Victims versus Jurors."]},{"cell_type":"code","metadata":{"id":"u7WMkhwX8wTc"},"source":["model_conditions = hddm.HDDM(\n","    # Note that we're now including data from both victims and jurors\n","    data.query('phase == \"alone\"'),\n","    bias=True,\n","    # This part is new!\n","    # The column 'study' contains the values 'Victim' and 'Juror'\n","    # So separate drift rates will be estimated for victims/jurors\n","    depends_on={'v': 'study'}\n",")\n","\n","model_conditions.find_starting_values()\n","\n","model_conditions.sample(50, burn=0, dbname=basepath+'/models/conditions_traces.db', db='pickle')\n","model_conditions.save(basepath+'/models/conditions_traces')\n","model_conditions.get_traces().to_csv(basepath+'/results/conditions_traces.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vY8dyLDz-Eu9"},"source":["As usual, you *must* check for convergence before hypothesis testing. We're going to skip this step for the sake of the tutorial, but remember that this is a critical and necessary part of statistical inference.\n","\n","Let's take a look at what hypothesis testing might look like in this setup. We might have the hypothesis that Jurors have higher preference for punishment than Victims. (Does this sound counterintuitive? [Check out this paper by my PhD supervisor](https://www.nature.com/articles/ncomms6306) if you're interested in learning more about the topic!)"]},{"cell_type":"code","metadata":{"id":"VDIX3WA0-n6f"},"source":["# Pull out the drift rate parameters\n","conditions_v_victim = model_conditions.nodes_db.node['v(Victim)']\n","conditions_v_juror = model_conditions.nodes_db.node['v(Juror)']\n","\n","# Plot posteriors\n","hddm.analyze.plot_posterior_nodes([conditions_v_victim, conditions_v_juror])\n","\n","# Test hypothesis\n","print(\n","    (conditions_v_victim.trace() < conditions_v_juror.trace()).mean()\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e85VvMF11ph7"},"source":["# Regression (within-subject)\n","\n","Regression is my favorite part of using HDDM, and the reason why I anticipate HDDM being the gold standard software for many years to come.\n","\n","The downsides of using the `depends_on` syntax in vanilla HDDM are:\n","\n","1. Only works if you have discrete conditions. What if you had trial-by-trial manipulations that you want to analyze, or trial-by-trial biological measures?\n","\n","2. If you have within-subject conditions, using `depends_on` is conceptually the same as running X separate models for X conditions. Therefore, you lose all of the benefits of running a within-subject study.\n","\n","In the context of my study, I might hypothesize that groups provide stronger evidence for punishment's value when there are increasingly large majorities who endorse punishment. Not too controversial, I think. Since we have a condition where participants are choosing alone, let's use that as the \"reference category\" against which we'll compare other conditions."]},{"cell_type":"code","metadata":{"id":"AW8yQheSGrP3"},"source":["model_regression = hddm.HDDMRegressor(\n","    # For tutorial, only looking at victim data\n","    data.query('study == \"Victim\" & offer == \"unfair\"'),\n","    # Define regression models of arbitrary complexity, for any parameter\n","    {\"v ~ C(rev_cat, Treatment('Alone'))\",\n","     \"a ~ C(rev_cat, Treatment('Alone'))\"},\n","     group_only_regressors=True,\n","     p_outlier=.05,\n","     include={'z'}\n",")\n","\n","model_regression.find_starting_values()\n","\n","model_regression.sample(10, burn=0, dbname=basepath+'/models/regression_traces.db', db='pickle')\n","model_regression.save(basepath+'/models/regression_traces')\n","model_regression.get_traces().to_csv(basepath+'/results/regression_traces.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xG8cPBiSIyra"},"source":["Although you need to perform convergence checks before interpeting results, we're again skipping those for the sake of time.\n","\n","We can do a quick set of hypothesis tests, as usual."]},{"cell_type":"code","metadata":{"id":"fNrLArgOJv7l"},"source":["regression_v_0 = model_regression.nodes_db.node['v_C(rev_cat, Treatment(\\'Alone\\'))[T.Zero]']\n","regression_v_1 = model_regression.nodes_db.node['v_C(rev_cat, Treatment(\\'Alone\\'))[T.One]']\n","regression_v_2 = model_regression.nodes_db.node['v_C(rev_cat, Treatment(\\'Alone\\'))[T.Two]']\n","regression_v_3 = model_regression.nodes_db.node['v_C(rev_cat, Treatment(\\'Alone\\'))[T.Three]']\n","regression_v_4 = model_regression.nodes_db.node['v_C(rev_cat, Treatment(\\'Alone\\'))[T.Four]']\n","\n","# Plot posteriors\n","hddm.analyze.plot_posterior_nodes(\n","    [\n","     regression_v_0,\n","     regression_v_1,\n","     regression_v_2,\n","     regression_v_3,\n","     regression_v_4\n","    ]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"reeTrGRLLPTe"},"source":["# Test hypothesis that drift rate is stronger when 4/4 group members endorse\n","# punishment, compared to when 0/4 group members endorse punishment\n","print(\n","    (regression_v_0.trace() < regression_v_4.trace()).mean()\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kwLzKfUjOf1I"},"source":["## Model comparison\n","\n","Our regression model specification was fairly complex. Does it outperform a simpler model? Are there other regression models that can test alternative hypotheses?\n","\n","This is where model comparison comes in play. Trivially, we need at least two models to compare, so let's estimate a less complex regression model where only the drift rate is modulated by a predictor."]},{"cell_type":"code","metadata":{"id":"Zd7EhelJPNu4"},"source":["model_comparison = hddm.HDDMRegressor(\n","    # For tutorial, only looking at victim data\n","    data.query('study == \"Victim\" & offer == \"unfair\"'),\n","    # Get rid of threshold regression\n","    {\"v ~ C(rev_cat, Treatment('Alone'))\"},\n","     group_only_regressors=True,\n","     p_outlier=.05,\n","     include={'z'}\n",")\n","\n","model_comparison.find_starting_values()\n","\n","model_comparison.sample(10, burn=0, dbname=basepath+'/models/comparison_traces.db', db='pickle')\n","model_comparison.save(basepath+'/models/comparison_traces')\n","model_comparison.get_traces().to_csv(basepath+'/results/comparison_traces.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8qcypcNQPx60"},"source":["The simplest heuristic we can use is DIC, a model fit statistic similar to AIC/BIC, but appropriate for hierarchical models like the ones we're estimating. Like AIC/BIC, lower numbers indicate better fit. Like BIC, this statistic penalizes for model complexity."]},{"cell_type":"code","metadata":{"id":"D9EY4BIc1sN6"},"source":["# Get DICs\n","print(\"DIC: %f\" %model_regression.dic)\n","print(\"DIC: %f\" %model_comparison.dic)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wiLEPD60QNI-"},"source":["Of course, there's a lot that isn't communicated by a single statistic... This is where posterior predictive checks can come in handy, as they might illuminate how the model is systematically mischaracterizing your data. Even if a particular model isn't doing the \"best\" job of minimizing overall error (has higher DIC), it might still be capturing patterns in your data that are being missed by the \"better\" model."]},{"cell_type":"markdown","metadata":{"id":"G3xz830CQsoS"},"source":["## Posterior predictive check\n","\n","PPCs can (and should) be performed for any class of HDDM models, but we'll focus our attention on what they look like in regression models...\n","\n","(Look at CSV)\n","\n","(Look at [Supplementary Information](https://static-content.springer.com/esm/art%3A10.1038%2Fs41598-019-48050-2/MediaObjects/41598_2019_48050_MOESM1_ESM.docx) for Crowdsourcing Punishment paper, particularly Figures S7 and S9)"]},{"cell_type":"code","metadata":{"id":"2Rcv4i86RJWf"},"source":["# Generate predicted results from previously-estimated models\n","# Normally, we'd want to get many, many samples - I've done 1000 in the past\n","# But, for the sake of time, let's just do a couple\n","(\n","    post_pred_gen(model_regression, samples=2, append_data=True)\n","    .to_csv(basepath+'/ppc/regression_ppc.csv')\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EMRtwNgZVn2J"},"source":["# Parameter recovery\n","\n","Depending on your task and how participants respond to it, you might end up estimating values from \"weird\" parts of parameter space. Even if multiple chains converge on the (more-or-less) same parameter values, those values might not be totally identifiable.\n","\n","Even harder to diagnose are cases when a particular *combination* of parameter estimates is un-identifiable.\n","\n","So, you want to make sure that HDDM is capable of recovering (accurately estimating) a set of \"ground truth\" parameters."]},{"cell_type":"markdown","metadata":{"id":"YLgWSYrKWtpY"},"source":["## Simulate data\n","\n","So, you've estimated some parameters from real data. Let's say that you've found the \"true\" parameters. If you were to run many more experiments, where the true parameter values remain the same, how reliably can you estimate those parameters in those new experiments?\n","\n","To answer this, we're going to simulate data based on the parameters estimated from real data."]},{"cell_type":"code","metadata":{"id":"iCrIVQS6Wo57"},"source":["# Number of subjects matches that of the actual experiment\n","# For the number of trials per condition, you should run this twice:\n","#      Once with a small number of trials (or, the number of trials in your experiment), which lets you know whether\n","#           HDDM can reliably estimate the parameters given the amount of data you originally fed it\n","#      Once with a larger number of trials, which lets you know whether HDDM can reliably estimate the parameters as\n","#           the amount of data used to inform the computation approaches infinity\n","num_subs = 40\n","trials_per_cond = 20\n","\n","# Drift rate estimates (empirical)\n","v_A = -0.2006909\n","v_0 = v_A + -0.7693828\n","v_1 = v_A + -0.6429486\n","v_2 = v_A + -0.1640435\n","v_3 = v_A + 0.4457329\n","v_4 = v_A + 0.6793762\n","\n","# Threshold estimates\n","a_A = 2.1356529\n","a_0 = a_A + -0.1365446\n","a_1 = a_A + -0.1131092\n","a_2 = a_A + 0.0615770\n","a_3 = a_A + -0.0230197\n","a_4 = a_A + -0.1731338\n","\n","# Bias and nondecision time estimates\n","z = 0.4410610\n","t = 0.3206254\n","\n","# Define each condition and its corresponding parameters\n","params = {\n","    'rev_A': {'a':a_A, 't':t, 'v':v_A, 'z':z},\n","    'rev_0': {'a':a_0, 't':t, 'v':v_0, 'z':z},\n","    'rev_1': {'a':a_1, 't':t, 'v':v_1, 'z':z},\n","    'rev_2': {'a':a_2, 't':t, 'v':v_2, 'z':z},\n","    'rev_3': {'a':a_3, 't':t, 'v':v_3, 'z':z},\n","    'rev_4': {'a':a_4, 't':t, 'v':v_4, 'z':z}\n","}\n","\n","# Now simulate that data\n","data, params = hddm.generate.gen_rand_data(\n","    params,\n","    size=trials_per_cond,\n","    subjs=num_subs,\n","    subj_noise=.1\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bpBzqDB5Xiwh"},"source":["## Estimate parameters from simulated data\n","\n","Literally just copy/paste the regression model you used to estimate parameters.\n","\n","The point of doing this analysis is to make sure that the combination of parameters you estimated from the empirical data are parameters that HDDM is capable of reliably estimating.\n","\n","So given that we know exactly what the expected parameter values are (since we simulated them), can HDDM then take that simulated data and give us an estimate that's in the right ballpark?"]},{"cell_type":"code","metadata":{"id":"ethn8-jDXhyU"},"source":["# Create empty array that will eventually store our models\n","models_recovery = []\n","\n","# Loop over 5 times to get 5 chains\n","for i in range(5):\n","    # Define regression model\n","    model_recovery = hddm.HDDMRegressor(\n","        data,\n","        {\"v ~ C(condition, Treatment('rev_A'))\",\n","         \"a ~ C(condition, Treatment('rev_A'))\"},\n","         group_only_regressors=True,\n","         p_outlier=.05,\n","         include={'z'})\n","    model_recovery.find_starting_values()\n","    \n","    model_recovery.sample(10, burn=0, dbname=basepath+'/models/recovery_traces_%s.db'%i, db='pickle')\n","    model_recovery.savePatch(basepath+'/models/recovery_traces_%s.db'%i)\n","    \n","    # Once you're finished running a chain, add that chain to your array of models\n","    models_recovery.append(m_recovery)\n","\n","# Save traces to CSV\n","(\n","    concat_models(models_recovery)\n","    .get_traces()\n","    .to_csv(basepath+'/results/recovery_traces.csv')\n",")\n","\n","# Calculate Gelman-Rubin r-hat statistic\n","(\n","    pd.DataFrame.from_dict(gelman_rubin(models_recovery), orient='index')\n","    .to_csv(basepath+'/results/recovery_r_hat.csv')\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pCdXqpdjYssZ"},"source":["## Then what?\n","\n","At the very least, you want to ensure that the \"true\" parameter value (the one you used to simulate data) is within the Bayesian High Density Interval (HDI) corresponding to some credible interval of your choosing (95% often gets used by convention).\n","\n","If your known parameter value falls outside the HDI, then there's evidence that your parameters are drawn from a weird part of parameter space. This would be a good sign that you need to go back into your (regression) models and diagnose what's causing the weirdness."]}]}